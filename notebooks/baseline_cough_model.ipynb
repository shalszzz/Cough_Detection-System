{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t28R-zhnhw1c",
        "outputId": "351351f2-4c38-4f76-e6f3-fdc6cd4619f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH = \"/content/drive/MyDrive/cough_detection\"\n",
        "\n",
        "ZIP_PATH = PROJECT_PATH + \"/Multimodal_Cough_Dataset.zip\"\n",
        "\n",
        "EXTRACT_PATH = PROJECT_PATH + \"/dataset\""
      ],
      "metadata": {
        "id": "gOn12AMPkbB8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(EXTRACT_PATH)\n",
        "\n",
        "print(\"Extraction complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK4x-4Yzky5s",
        "outputId": "56a265fe-8bc4-42e8-c203-3f4283954ec4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir(EXTRACT_PATH)\n",
        "DATASET_PATH = EXTRACT_PATH + \"/Multimodal Cough Dataset\"\n",
        "\n",
        "os.listdir(DATASET_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8jEBMVZk3pK",
        "outputId": "7cfaf5f9-7306-43b3-eea3-e1930c1d9f0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['005',\n",
              " '006',\n",
              " '007',\n",
              " '008',\n",
              " '009',\n",
              " '010',\n",
              " '011',\n",
              " '012',\n",
              " '013',\n",
              " '014',\n",
              " '015',\n",
              " '016',\n",
              " '017',\n",
              " 'DataAnnotation.json']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eClD8WXmVS8",
        "outputId": "1d01d35c-7570-4d6c-ae72-3cb01cfabae8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.9.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (26.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.9.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix"
      ],
      "metadata": {
        "id": "E_IcbOmpmX4A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATION_PATH = DATASET_PATH + \"/DataAnnotation.json\"\n",
        "\n",
        "with open(ANNOTATION_PATH) as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "print(len(annotations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9vsfoE-maOI",
        "outputId": "85e14815-f7e8-4222-9a33-f80966da9702"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cough_dict = {}\n",
        "\n",
        "for record in annotations:\n",
        "\n",
        "    filename = record[\"original_filename\"].lower()\n",
        "\n",
        "    cough_dict[filename] = []\n",
        "\n",
        "    for seg in record[\"segmentations\"]:\n",
        "\n",
        "        if \"Cough\" in seg[\"annotations\"]:\n",
        "\n",
        "            cough_dict[filename].append(\n",
        "                (seg[\"start_time\"], seg[\"end_time\"])\n",
        "            )\n",
        "\n",
        "print(\"Total annotated files:\", len(cough_dict))\n",
        "\n",
        "example = list(cough_dict.keys())[0]\n",
        "\n",
        "print(example, cough_dict[example][:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G2yw0W9oWNV",
        "outputId": "a29fde04-06e9-4eeb-b7a1-73bdec6ba77b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total annotated files: 39\n",
            "005_no_talking_in.wav [(228.016, 228.366), (399.856, 400.156), (568.497, 568.893)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "sr_target = 750\n",
        "\n",
        "for participant in os.listdir(DATASET_PATH):\n",
        "\n",
        "    participant_path = os.path.join(DATASET_PATH, participant)\n",
        "\n",
        "    if not os.path.isdir(participant_path):\n",
        "        continue\n",
        "\n",
        "\n",
        "    for trial_folder in os.listdir(participant_path):\n",
        "\n",
        "        trial_path = os.path.join(participant_path, trial_folder)\n",
        "\n",
        "\n",
        "        # FIX: Skip files like sync_time.txt\n",
        "        if not os.path.isdir(trial_path):\n",
        "            continue\n",
        "\n",
        "\n",
        "        for file in os.listdir(trial_path):\n",
        "\n",
        "            if not file.endswith(\".wav\"):\n",
        "                continue\n",
        "\n",
        "\n",
        "            actual_filename = file.lower()\n",
        "\n",
        "            if actual_filename not in cough_dict:\n",
        "                continue\n",
        "\n",
        "\n",
        "            audio_path = os.path.join(trial_path, file)\n",
        "\n",
        "            audio, sr = librosa.load(audio_path, sr=sr_target)\n",
        "\n",
        "            cough_times = cough_dict[actual_filename]\n",
        "\n",
        "\n",
        "            window_size = int(2 * sr)\n",
        "            step_size = int(0.5 * sr)\n",
        "\n",
        "\n",
        "            for start in range(0, len(audio)-window_size, step_size):\n",
        "\n",
        "                start_sec = start / sr\n",
        "                end_sec = (start+window_size) / sr\n",
        "\n",
        "\n",
        "                label = 0\n",
        "\n",
        "                for cough_start, cough_end in cough_times:\n",
        "\n",
        "                    if not (end_sec < cough_start or start_sec > cough_end):\n",
        "\n",
        "                        label = 1\n",
        "                        break\n",
        "\n",
        "\n",
        "                mel = librosa.feature.melspectrogram(\n",
        "\n",
        "                    y=audio[start:start+window_size],\n",
        "                    sr=sr,\n",
        "                    n_mels=64,\n",
        "                    n_fft=256,\n",
        "                    hop_length=64\n",
        "                )\n",
        "\n",
        "                log_mel = librosa.power_to_db(mel)\n",
        "\n",
        "                X.append(log_mel.flatten())\n",
        "                y.append(label)\n",
        "\n",
        "\n",
        "print(\"Total samples:\", len(X))\n",
        "\n",
        "print(\"Cough samples:\", sum(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epq_NM75ogxm",
        "outputId": "88fda3f5-bdb0-40e8-dec6-945daa49d302"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 57566\n",
            "Cough samples: 10231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Feature shape:\", X.shape)\n",
        "print(\"Label shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhLGukaepwNX",
        "outputId": "1ce94a43-0736-44b9-b2a6-8038377e7fca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: (57566, 1536)\n",
            "Label shape: (57566,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split training and testing"
      ],
      "metadata": {
        "id": "2AeuCActp4Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\n",
        "    X, y,\n",
        "\n",
        "    test_size=0.2,\n",
        "\n",
        "    stratify=y,\n",
        "\n",
        "    random_state=42\n",
        "\n",
        ")\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Testing samples:\", X_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQWDXZN5p7Gm",
        "outputId": "228c0532-805e-49d5-a1dc-ff675801488f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 46052\n",
            "Testing samples: 11514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling"
      ],
      "metadata": {
        "id": "mtNfy-URqYab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "3yg-zPcVqaCx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "qNVaw9FPqjjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "baseline_model = LogisticRegression(\n",
        "\n",
        "    max_iter=1000,\n",
        "\n",
        "    C=0.1,\n",
        "\n",
        "    random_state=42\n",
        "\n",
        ")\n",
        "\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Baseline model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MINckcylqjAY",
        "outputId": "72c9763d-de07-4200-f9ce-55c011b054c5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "ZmhAFvfOrNbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Predict labels\n",
        "y_pred = baseline_model.predict(X_test)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = baseline_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Baseline Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Baseline AUC:\", auc)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxC5iDxPq-97",
        "outputId": "091d7fb7-b5d2-4c72-e3ef-4228c54cdf6f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.8534827166927219\n",
            "Baseline AUC: 0.8655578950715711\n",
            "\n",
            "Confusion Matrix:\n",
            "[[8971  497]\n",
            " [1190  856]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving to Drive"
      ],
      "metadata": {
        "id": "RXVtySNerKVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save baseline model\n",
        "pickle.dump(\n",
        "    baseline_model,\n",
        "    open(PROJECT_PATH + \"/baseline_model.pkl\", \"wb\")\n",
        ")\n",
        "\n",
        "# Save scaler\n",
        "pickle.dump(\n",
        "    scaler,\n",
        "    open(PROJECT_PATH + \"/baseline_scaler.pkl\", \"wb\")\n",
        ")\n",
        "\n",
        "print(\"Baseline model and scaler saved to Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFgNyCD4rRpP",
        "outputId": "50b5f986-5231-419f-edcb-e771972d7b16"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model and scaler saved to Google Drive\n"
          ]
        }
      ]
    }
  ]
}