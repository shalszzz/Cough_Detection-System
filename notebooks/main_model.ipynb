{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae30f406-7fc0-4453-b4d9-e9ba32e591a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents: ['008', '017', '016', '007', '014', '012', '015', 'DataAnnotation.json', '011', '010', '005', '013', '006', '009']\n",
      "\n",
      "✅ Found 39 annotations\n",
      "\n",
      "Sample: {'assigned_user': {'id': 3, 'role': 'admin', 'username': 'ychen239'}, 'created_at': 'Mon, 06 Nov 2023 21:27:29 GMT', 'filename': '2412d29e8da0442c9374dca64fb613f5.wav', 'is_marked_for_review': False, 'last_modified': 'Tue, 14 Nov 2023 19:04:32 GMT', 'original_filename': '005_no_talking_in.wav', 'reference_transcription': None, 'segmentations': [{'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:32:00 GMT', 'end_time': 228.366, 'last_modified': 'Mon, 13 Nov 2023 00:32:00 GMT', 'start_time': 228.016, 'transcription': None}, {'annotations': {'Other Sound': {'id': 9, 'values': {'id': 8, 'value': '8'}}}, 'created_at': 'Mon, 13 Nov 2023 00:32:09 GMT', 'end_time': 243.776, 'last_modified': 'Mon, 13 Nov 2023 00:32:09 GMT', 'start_time': 240.626, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:34:12 GMT', 'end_time': 400.156, 'last_modified': 'Mon, 13 Nov 2023 00:34:12 GMT', 'start_time': 399.856, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:37:29 GMT', 'end_time': 568.893, 'last_modified': 'Mon, 13 Nov 2023 00:37:29 GMT', 'start_time': 568.497, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:02 GMT', 'end_time': 752.101, 'last_modified': 'Mon, 13 Nov 2023 00:39:11 GMT', 'start_time': 751.684, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:05 GMT', 'end_time': 731.038, 'last_modified': 'Mon, 13 Nov 2023 00:39:09 GMT', 'start_time': 730.684, 'transcription': None}, {'annotations': {'Other Sound': {'id': 9, 'values': {'id': 8, 'value': '8'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:19 GMT', 'end_time': 346.016, 'last_modified': 'Mon, 13 Nov 2023 00:39:22 GMT', 'start_time': 340.256, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:29 GMT', 'end_time': 358.516, 'last_modified': 'Mon, 13 Nov 2023 00:39:29 GMT', 'start_time': 358.276, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:34 GMT', 'end_time': 379.236, 'last_modified': 'Mon, 13 Nov 2023 00:39:34 GMT', 'start_time': 378.996, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:41 GMT', 'end_time': 421.621, 'last_modified': 'Mon, 13 Nov 2023 00:39:41 GMT', 'start_time': 421.311, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:44 GMT', 'end_time': 441.821, 'last_modified': 'Mon, 13 Nov 2023 00:39:44 GMT', 'start_time': 441.571, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:48 GMT', 'end_time': 462.611, 'last_modified': 'Mon, 13 Nov 2023 00:39:48 GMT', 'start_time': 462.281, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:52 GMT', 'end_time': 506.421, 'last_modified': 'Mon, 13 Nov 2023 00:39:52 GMT', 'start_time': 506.151, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:40:41 GMT', 'end_time': 248.964, 'last_modified': 'Mon, 13 Nov 2023 00:40:41 GMT', 'start_time': 248.431, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:40:48 GMT', 'end_time': 269.597, 'last_modified': 'Mon, 13 Nov 2023 00:40:48 GMT', 'start_time': 269.164, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:40:55 GMT', 'end_time': 290.597, 'last_modified': 'Mon, 13 Nov 2023 00:40:55 GMT', 'start_time': 290.197, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:41:01 GMT', 'end_time': 311.297, 'last_modified': 'Mon, 13 Nov 2023 00:41:01 GMT', 'start_time': 310.964, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:41:55 GMT', 'end_time': 527.528, 'last_modified': 'Mon, 13 Nov 2023 00:41:55 GMT', 'start_time': 527.128, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:02 GMT', 'end_time': 548.394, 'last_modified': 'Mon, 13 Nov 2023 00:42:02 GMT', 'start_time': 547.861, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:14 GMT', 'end_time': 590.127, 'last_modified': 'Mon, 13 Nov 2023 00:42:14 GMT', 'start_time': 589.694, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:23 GMT', 'end_time': 610.86, 'last_modified': 'Mon, 13 Nov 2023 00:42:23 GMT', 'start_time': 610.394, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:33 GMT', 'end_time': 648.227, 'last_modified': 'Mon, 13 Nov 2023 00:42:33 GMT', 'start_time': 647.827, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:42 GMT', 'end_time': 668.46, 'last_modified': 'Mon, 13 Nov 2023 00:42:42 GMT', 'start_time': 668.193, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:49 GMT', 'end_time': 689.726, 'last_modified': 'Mon, 13 Nov 2023 00:42:49 GMT', 'start_time': 689.326, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:56 GMT', 'end_time': 710.459, 'last_modified': 'Mon, 13 Nov 2023 00:42:56 GMT', 'start_time': 710.026, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:25 GMT', 'end_time': 55.1645, 'last_modified': 'Mon, 13 Nov 2023 00:44:25 GMT', 'start_time': 54.8958, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:31 GMT', 'end_time': 75.8362, 'last_modified': 'Mon, 13 Nov 2023 00:44:31 GMT', 'start_time': 75.5825, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:40 GMT', 'end_time': 96.6273, 'last_modified': 'Mon, 13 Nov 2023 00:44:40 GMT', 'start_time': 96.3139, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:49 GMT', 'end_time': 117.613, 'last_modified': 'Mon, 13 Nov 2023 00:44:49 GMT', 'start_time': 117.374, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:57 GMT', 'end_time': 138.18, 'last_modified': 'Mon, 13 Nov 2023 00:44:57 GMT', 'start_time': 137.911, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:45:07 GMT', 'end_time': 158.881, 'last_modified': 'Mon, 13 Nov 2023 00:45:07 GMT', 'start_time': 158.613, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:45:17 GMT', 'end_time': 207.583, 'last_modified': 'Mon, 13 Nov 2023 00:45:21 GMT', 'start_time': 207.344, 'transcription': None}], 'url': '/audios/2412d29e8da0442c9374dca64fb613f5.wav'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36918/2539373233.py:20: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_path, sr=750)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Multimodal Cough Dataset/005/Trial_1_No_Talking/005_In.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mSoundFile(path)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(file, mode_int, closefd)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'Multimodal Cough Dataset/005/Trial_1_No_Talking/005_In.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Check a sample audio file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/005/Trial_1_No_Talking/005_In.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m750\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Loaded audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sample rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Hz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m __audioread_load(path, offset, duration, dtype)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/librosa/util/decorators.py:63\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/librosa/core/audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m audioread\u001b[38;5;241m.\u001b[39maudio_open(path)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/audioread/__init__.py:126\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BackendClass(path)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Multimodal Cough Dataset/005/Trial_1_No_Talking/005_In.wav'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Check what's in the dataset folder\n",
    "dataset_path = \"Multimodal Cough Dataset\"\n",
    "print(\"Contents:\", os.listdir(dataset_path))\n",
    "\n",
    "# Load the annotations\n",
    "with open(f\"{dataset_path}/DataAnnotation.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(f\"\\n✅ Found {len(annotations)} annotations\")\n",
    "print(\"\\nSample:\", annotations[0])\n",
    "\n",
    "# Check a sample audio file\n",
    "audio_path = f\"{dataset_path}/005/Trial_1_No_Talking/005_In.wav\"\n",
    "audio, sr = librosa.load(audio_path, sr=750)\n",
    "print(f\"\\n✅ Loaded audio: {audio_path}\")\n",
    "print(f\"Audio shape: {audio.shape}, Sample rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44d0e61-b24c-42ef-8493-2c1a734847d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: pandas in /home/devgokul/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /home/devgokul/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/devgokul/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/devgokul/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in /home/devgokul/anaconda3/lib/python3.11/site-packages (1.11.4)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (1.9.0)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/devgokul/anaconda3/lib/python3.11/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.78.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /home/devgokul/anaconda3/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: pycparser in /home/devgokul/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/devgokul/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.6/620.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.4.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.78.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.6/242.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (400 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.8/400.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, soxr, protobuf, optree, opt_einsum, ml_dtypes, h5py, grpcio, google_pasta, gast, audioread, astunparse, absl-py, tensorboard, soundfile, librosa, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.30.0 requires protobuf<5,>=3.20, but you have protobuf 6.33.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.4.0 astunparse-1.6.3 audioread-3.1.0 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.78.0 h5py-3.15.1 keras-3.13.2 libclang-18.1.1 librosa-0.11.0 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 protobuf-6.33.5 soundfile-0.13.1 soxr-1.0.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa pandas numpy matplotlib tensorflow scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e31e89-f112-4416-b124-80186a225bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does path exist? False\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Multimodal Cough Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if path exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoes path exist?\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path))\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContents:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mlistdir(dataset_path))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# For file access, use proper path joining\u001b[39;00m\n\u001b[1;32m     14\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m005\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial_1_No_Talking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m005_In.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Multimodal Cough Dataset'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Use raw string or escape spaces\n",
    "dataset_path = \"Multimodal Cough Dataset\"  # This has a space!\n",
    "\n",
    "# Check if path exists\n",
    "print(\"Does path exist?\", os.path.exists(dataset_path))\n",
    "print(\"Contents:\", os.listdir(dataset_path))\n",
    "\n",
    "# For file access, use proper path joining\n",
    "audio_path = os.path.join(dataset_path, \"005\", \"Trial_1_No_Talking\", \"005_In.wav\")\n",
    "print(\"Full path:\", audio_path)\n",
    "print(\"File exists?\", os.path.exists(audio_path))\n",
    "\n",
    "# Now load\n",
    "if os.path.exists(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=750)\n",
    "    print(f\"✅ Success! Audio shape: {audio.shape}\")\n",
    "else:\n",
    "    print(\"❌ File still not found - check the path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c13874-02dd-4100-870b-31451c7ff3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/devgokul/shalini/cough-vagus-hackathon\n",
      "Folders here: ['dataset', '.vscode', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Folders here:\", [f for f in os.listdir('.') if os.path.isdir(f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734e272f-d0eb-4912-bbfc-b778c012f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does dataset exist? True\n",
      "Contents: ['008', '017', '016', '007', '014', '012', '015', 'DataAnnotation.json', '011', '010', '005', '013', '006', '009']\n",
      "Participants found: ['008', '017', '016', '007', '014', '012', '015', '011', '010', '005', '013', '006', '009']\n",
      "Trying: dataset/005/Trial_1_No_Talking/005_In.wav\n",
      "File exists? False\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset\"\n",
    "print(\"Does dataset exist?\", os.path.exists(dataset_path))\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Contents:\", os.listdir(dataset_path))\n",
    "    \n",
    "    # Check participant folders\n",
    "    participants = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d)) and d.isdigit()]\n",
    "    print(f\"Participants found: {participants}\")\n",
    "    \n",
    "    if participants:\n",
    "        # Try loading a sample\n",
    "        audio_path = f\"{dataset_path}/005/Trial_1_No_Talking/005_In.wav\"\n",
    "        print(f\"Trying: {audio_path}\")\n",
    "        print(\"File exists?\", os.path.exists(audio_path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c399feb-c2d9-47ac-9601-449b5f546bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of dataset/005:\n",
      "  - Trial_1_No_Talking\n",
      "    Inside Trial_1_No_Talking:\n",
      "      - Magnetometer.csv\n",
      "      - Accelerometer.csv\n",
      "      - 005_No_Talking_In.wav\n",
      "      - 005_No_Talking_Out.wav\n",
      "      - Gyroscope.csv\n",
      "  - sync_time.txt\n",
      "  - Trial_2_Talking\n",
      "    Inside Trial_2_Talking:\n",
      "      - Magnetometer.csv\n",
      "      - 005_Talking_In.wav\n",
      "      - Accelerometer.csv\n",
      "      - 005_Talking_Out.wav\n",
      "      - Gyroscope.csv\n",
      "  - Trial_3_Nonverbal\n",
      "    Inside Trial_3_Nonverbal:\n",
      "      - Magnetometer.csv\n",
      "      - Accelerometer.csv\n",
      "      - 005_Nonverbal_In.wav\n",
      "      - 005_Nonverbal_Out.wav\n",
      "      - Gyroscope.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = \"dataset\"\n",
    "participant = \"005\"\n",
    "\n",
    "# List everything in participant folder\n",
    "participant_path = os.path.join(dataset_path, participant)\n",
    "print(f\"Contents of {participant_path}:\")\n",
    "for item in os.listdir(participant_path):\n",
    "    print(f\"  - {item}\")\n",
    "    \n",
    "    # If it's a folder, look inside\n",
    "    item_path = os.path.join(participant_path, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"    Inside {item}:\")\n",
    "        for subitem in os.listdir(item_path):\n",
    "            print(f\"      - {subitem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84543f5e-3dd3-4321-b7cd-0a045867ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio in path: dataset/005/Trial_1_No_Talking/005_No_Talking_In.wav\n",
      "File exists? True\n",
      "✅ Success! Audio loaded: (590544,) samples\n",
      "✅ Accelerometer loaded: (95123, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"dataset\"\n",
    "participant = \"005\"\n",
    "trial = \"Trial_1_No_Talking\"\n",
    "\n",
    "# Correct filename pattern\n",
    "audio_in_path = os.path.join(dataset_path, participant, trial, f\"{participant}_No_Talking_In.wav\")\n",
    "audio_out_path = os.path.join(dataset_path, participant, trial, f\"{participant}_No_Talking_Out.wav\")\n",
    "\n",
    "print(f\"Audio in path: {audio_in_path}\")\n",
    "print(f\"File exists? {os.path.exists(audio_in_path)}\")\n",
    "\n",
    "if os.path.exists(audio_in_path):\n",
    "    audio, sr = librosa.load(audio_in_path, sr=750)\n",
    "    print(f\"✅ Success! Audio loaded: {audio.shape} samples\")\n",
    "    \n",
    "    # Load corresponding IMU\n",
    "    accel_path = os.path.join(dataset_path, participant, trial, \"Accelerometer.csv\")\n",
    "    if os.path.exists(accel_path):\n",
    "        import pandas as pd\n",
    "        accel = pd.read_csv(accel_path)\n",
    "        print(f\"✅ Accelerometer loaded: {accel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee204ed-29a9-4884-a207-f3032687dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial_1_No_Talking: True\n",
      "Trial_2_Talking: True\n",
      "Trial_3_Nonverbal: True\n"
     ]
    }
   ],
   "source": [
    "for trial in [\"Trial_1_No_Talking\", \"Trial_2_Talking\", \"Trial_3_Nonverbal\"]:\n",
    "    # Map trial to filename pattern\n",
    "    if trial == \"Trial_1_No_Talking\":\n",
    "        pattern = \"No_Talking\"\n",
    "    elif trial == \"Trial_2_Talking\":\n",
    "        pattern = \"Talking\"\n",
    "    else:\n",
    "        pattern = \"Nonverbal\"\n",
    "    \n",
    "    audio_path = f\"{dataset_path}/005/{trial}/005_{pattern}_In.wav\"\n",
    "    print(f\"{trial}: {os.path.exists(audio_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2fe73c-a99b-487b-ad72-9812c9b0d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###till here we've checked if the dataset exists and if all our files exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcac7e05-db42-4970-9067-e6514c163009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations: 39\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Count by label type\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m---> 14\u001b[0m label_counts \u001b[38;5;241m=\u001b[39m Counter([ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m annotations])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLabel distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, count \u001b[38;5;129;01min\u001b[39;00m label_counts\u001b[38;5;241m.\u001b[39mmost_common():\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Count by label type\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m---> 14\u001b[0m label_counts \u001b[38;5;241m=\u001b[39m Counter([ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m annotations])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLabel distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, count \u001b[38;5;129;01min\u001b[39;00m label_counts\u001b[38;5;241m.\u001b[39mmost_common():\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load annotations\n",
    "with open(f\"{dataset_path}/DataAnnotation.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(f\"Total annotations: {len(annotations)}\")\n",
    "\n",
    "# Count by label type\n",
    "from collections import Counter\n",
    "label_counts = Counter([ann['label'] for ann in annotations])\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label, count in label_counts.most_common():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacb2d55-212f-480d-930f-e5ec7569b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006877db-9666-4519-9758-59b04bdc2be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of annotations: <class 'list'>\n",
      "\n",
      "First item (if list):\n",
      "{'assigned_user': {'id': 3, 'role': 'admin', 'username': 'ychen239'}, 'created_at': 'Mon, 06 Nov 2023 21:27:29 GMT', 'filename': '2412d29e8da0442c9374dca64fb613f5.wav', 'is_marked_for_review': False, 'last_modified': 'Tue, 14 Nov 2023 19:04:32 GMT', 'original_filename': '005_no_talking_in.wav', 'reference_transcription': None, 'segmentations': [{'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:32:00 GMT', 'end_time': 228.366, 'last_modified': 'Mon, 13 Nov 2023 00:32:00 GMT', 'start_time': 228.016, 'transcription': None}, {'annotations': {'Other Sound': {'id': 9, 'values': {'id': 8, 'value': '8'}}}, 'created_at': 'Mon, 13 Nov 2023 00:32:09 GMT', 'end_time': 243.776, 'last_modified': 'Mon, 13 Nov 2023 00:32:09 GMT', 'start_time': 240.626, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:34:12 GMT', 'end_time': 400.156, 'last_modified': 'Mon, 13 Nov 2023 00:34:12 GMT', 'start_time': 399.856, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:37:29 GMT', 'end_time': 568.893, 'last_modified': 'Mon, 13 Nov 2023 00:37:29 GMT', 'start_time': 568.497, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:02 GMT', 'end_time': 752.101, 'last_modified': 'Mon, 13 Nov 2023 00:39:11 GMT', 'start_time': 751.684, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:05 GMT', 'end_time': 731.038, 'last_modified': 'Mon, 13 Nov 2023 00:39:09 GMT', 'start_time': 730.684, 'transcription': None}, {'annotations': {'Other Sound': {'id': 9, 'values': {'id': 8, 'value': '8'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:19 GMT', 'end_time': 346.016, 'last_modified': 'Mon, 13 Nov 2023 00:39:22 GMT', 'start_time': 340.256, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:29 GMT', 'end_time': 358.516, 'last_modified': 'Mon, 13 Nov 2023 00:39:29 GMT', 'start_time': 358.276, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:34 GMT', 'end_time': 379.236, 'last_modified': 'Mon, 13 Nov 2023 00:39:34 GMT', 'start_time': 378.996, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:41 GMT', 'end_time': 421.621, 'last_modified': 'Mon, 13 Nov 2023 00:39:41 GMT', 'start_time': 421.311, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:44 GMT', 'end_time': 441.821, 'last_modified': 'Mon, 13 Nov 2023 00:39:44 GMT', 'start_time': 441.571, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:48 GMT', 'end_time': 462.611, 'last_modified': 'Mon, 13 Nov 2023 00:39:48 GMT', 'start_time': 462.281, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:39:52 GMT', 'end_time': 506.421, 'last_modified': 'Mon, 13 Nov 2023 00:39:52 GMT', 'start_time': 506.151, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:40:41 GMT', 'end_time': 248.964, 'last_modified': 'Mon, 13 Nov 2023 00:40:41 GMT', 'start_time': 248.431, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:40:48 GMT', 'end_time': 269.597, 'last_modified': 'Mon, 13 Nov 2023 00:40:48 GMT', 'start_time': 269.164, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:40:55 GMT', 'end_time': 290.597, 'last_modified': 'Mon, 13 Nov 2023 00:40:55 GMT', 'start_time': 290.197, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:41:01 GMT', 'end_time': 311.297, 'last_modified': 'Mon, 13 Nov 2023 00:41:01 GMT', 'start_time': 310.964, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:41:55 GMT', 'end_time': 527.528, 'last_modified': 'Mon, 13 Nov 2023 00:41:55 GMT', 'start_time': 527.128, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:02 GMT', 'end_time': 548.394, 'last_modified': 'Mon, 13 Nov 2023 00:42:02 GMT', 'start_time': 547.861, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:14 GMT', 'end_time': 590.127, 'last_modified': 'Mon, 13 Nov 2023 00:42:14 GMT', 'start_time': 589.694, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:23 GMT', 'end_time': 610.86, 'last_modified': 'Mon, 13 Nov 2023 00:42:23 GMT', 'start_time': 610.394, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:33 GMT', 'end_time': 648.227, 'last_modified': 'Mon, 13 Nov 2023 00:42:33 GMT', 'start_time': 647.827, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:42 GMT', 'end_time': 668.46, 'last_modified': 'Mon, 13 Nov 2023 00:42:42 GMT', 'start_time': 668.193, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:49 GMT', 'end_time': 689.726, 'last_modified': 'Mon, 13 Nov 2023 00:42:49 GMT', 'start_time': 689.326, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:42:56 GMT', 'end_time': 710.459, 'last_modified': 'Mon, 13 Nov 2023 00:42:56 GMT', 'start_time': 710.026, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:25 GMT', 'end_time': 55.1645, 'last_modified': 'Mon, 13 Nov 2023 00:44:25 GMT', 'start_time': 54.8958, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:31 GMT', 'end_time': 75.8362, 'last_modified': 'Mon, 13 Nov 2023 00:44:31 GMT', 'start_time': 75.5825, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:40 GMT', 'end_time': 96.6273, 'last_modified': 'Mon, 13 Nov 2023 00:44:40 GMT', 'start_time': 96.3139, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:49 GMT', 'end_time': 117.613, 'last_modified': 'Mon, 13 Nov 2023 00:44:49 GMT', 'start_time': 117.374, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:44:57 GMT', 'end_time': 138.18, 'last_modified': 'Mon, 13 Nov 2023 00:44:57 GMT', 'start_time': 137.911, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:45:07 GMT', 'end_time': 158.881, 'last_modified': 'Mon, 13 Nov 2023 00:45:07 GMT', 'start_time': 158.613, 'transcription': None}, {'annotations': {'Cough': {'id': 1, 'values': {'id': 1, 'value': '0'}}}, 'created_at': 'Mon, 13 Nov 2023 00:45:17 GMT', 'end_time': 207.583, 'last_modified': 'Mon, 13 Nov 2023 00:45:21 GMT', 'start_time': 207.344, 'transcription': None}], 'url': '/audios/2412d29e8da0442c9374dca64fb613f5.wav'}\n",
      "\n",
      "Keys in first item: dict_keys(['assigned_user', 'created_at', 'filename', 'is_marked_for_review', 'last_modified', 'original_filename', 'reference_transcription', 'segmentations', 'url'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"{dataset_path}/DataAnnotation.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(\"Type of annotations:\", type(annotations))\n",
    "print(\"\\nFirst item (if list):\")\n",
    "if isinstance(annotations, list):\n",
    "    print(annotations[0])\n",
    "    print(\"\\nKeys in first item:\", annotations[0].keys())\n",
    "else:\n",
    "    print(annotations)\n",
    "    print(\"\\nKeys in dict:\", annotations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f205c4ed-aa02-4f62-aeae-942b355cf77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File loaded\n",
      "Type: <class 'list'>\n",
      "Number of items: 39\n"
     ]
    }
   ],
   "source": [
    "#load and explore annotations.json\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load the annotations\n",
    "with open(\"dataset/DataAnnotation.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(\"✅ File loaded\")\n",
    "print(f\"Type: {type(annotations)}\")\n",
    "print(f\"Number of items: {len(annotations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f417bf4-808e-4a32-9908-5b0ae7a35921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total events extracted: 5841\n",
      "\n",
      "📋 First 5 events:\n",
      "  1. Cough - 005 - 228.02s to 228.37s\n",
      "  2. Other Sound - 005 - 240.63s to 243.78s\n",
      "  3. Cough - 005 - 399.86s to 400.16s\n",
      "  4. Cough - 005 - 568.50s to 568.89s\n",
      "  5. Cough - 005 - 751.68s to 752.10s\n",
      "\n",
      "📊 Label distribution:\n",
      "  Speech: 2140\n",
      "  Cough: 2137\n",
      "  Deep breath: 565\n",
      "  Laugh: 269\n",
      "  Groan: 249\n",
      "  Speech (far): 207\n",
      "  Sneeze: 189\n",
      "  Other Sound: 85\n"
     ]
    }
   ],
   "source": [
    "#extract events from annotation\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load annotations\n",
    "with open(\"dataset/DataAnnotation.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Extract all events\n",
    "events = []\n",
    "for ann_idx, ann in enumerate(annotations):\n",
    "    filename = ann.get('original_filename', 'unknown')\n",
    "    participant = filename.split('_')[0] if filename != 'unknown' else 'unknown'\n",
    "    \n",
    "    # Get segmentations\n",
    "    segmentations = ann.get('segmentations', [])\n",
    "    for seg in segmentations:\n",
    "        # Get annotations inside segment\n",
    "        seg_ann = seg.get('annotations', {})\n",
    "        for label_name in seg_ann.keys():\n",
    "            events.append({\n",
    "                'participant': participant,\n",
    "                'filename': filename,\n",
    "                'label': label_name,\n",
    "                'start_time': seg.get('start_time', 0),\n",
    "                'end_time': seg.get('end_time', 0)\n",
    "            })\n",
    "\n",
    "print(f\"✅ Total events extracted: {len(events)}\")\n",
    "\n",
    "# Show first 5 events\n",
    "print(\"\\n📋 First 5 events:\")\n",
    "for i, e in enumerate(events[:5]):\n",
    "    print(f\"  {i+1}. {e['label']} - {e['participant']} - {e['start_time']:.2f}s to {e['end_time']:.2f}s\")\n",
    "\n",
    "# Count by label\n",
    "label_counts = Counter([e['label'] for e in events])\n",
    "print(\"\\n📊 Label distribution:\")\n",
    "for label, count in label_counts.most_common():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a893c771-cccf-434f-89a4-feda712c191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Events per participant:\n",
      "--------------------------------------------------\n",
      "Participant 005:\n",
      "  Coughs:  179 | Speech:  190 | Total:  437\n",
      "  Cough %: 41.0%\n",
      "\n",
      "Participant 006:\n",
      "  Coughs:  127 | Speech:  155 | Total:  400\n",
      "  Cough %: 31.8%\n",
      "\n",
      "Participant 007:\n",
      "  Coughs:  168 | Speech:  123 | Total:  331\n",
      "  Cough %: 50.8%\n",
      "\n",
      "Participant 008:\n",
      "  Coughs:  149 | Speech:  182 | Total:  424\n",
      "  Cough %: 35.1%\n",
      "\n",
      "Participant 009:\n",
      "  Coughs:  162 | Speech:  160 | Total:  374\n",
      "  Cough %: 43.3%\n",
      "\n",
      "Participant 010:\n",
      "  Coughs:   97 | Speech:  158 | Total:  348\n",
      "  Cough %: 27.9%\n",
      "\n",
      "Participant 011:\n",
      "  Coughs:  192 | Speech:  144 | Total:  509\n",
      "  Cough %: 37.7%\n",
      "\n",
      "Participant 012:\n",
      "  Coughs:  168 | Speech:  148 | Total:  472\n",
      "  Cough %: 35.6%\n",
      "\n",
      "Participant 013:\n",
      "  Coughs:  203 | Speech:  169 | Total:  531\n",
      "  Cough %: 38.2%\n",
      "\n",
      "Participant 014:\n",
      "  Coughs:  171 | Speech:  147 | Total:  537\n",
      "  Cough %: 31.8%\n",
      "\n",
      "Participant 015:\n",
      "  Coughs:  185 | Speech:  179 | Total:  473\n",
      "  Cough %: 39.1%\n",
      "\n",
      "Participant 016:\n",
      "  Coughs:  159 | Speech:  197 | Total:  512\n",
      "  Cough %: 31.1%\n",
      "\n",
      "Participant 017:\n",
      "  Coughs:  177 | Speech:  188 | Total:  493\n",
      "  Cough %: 35.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#analysing events per participant\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group events by participant and label\n",
    "participant_stats = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for e in events:\n",
    "    participant_stats[e['participant']][e['label']] += 1\n",
    "\n",
    "print(\"📊 Events per participant:\")\n",
    "print(\"-\" * 50)\n",
    "for participant in sorted(participant_stats.keys()):\n",
    "    stats = participant_stats[participant]\n",
    "    coughs = stats.get('Cough', 0)\n",
    "    speech = stats.get('Speech', 0)\n",
    "    total = sum(stats.values())\n",
    "    print(f\"Participant {participant}:\")\n",
    "    print(f\"  Coughs: {coughs:4d} | Speech: {speech:4d} | Total: {total:4d}\")\n",
    "    print(f\"  Cough %: {coughs/total*100:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0c8469-5c01-4f3a-996c-3fbbd344a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecaa4b1-ef66-4bba-b2c7-321c47fdeb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training dataset in batches...\n",
      "Total events to process: 5841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 59/59 [00:00<00:00, 4169.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset built successfully!\n",
      "   X shape: (0,)\n",
      "   y shape: (0,)\n",
      "   Cough samples: 0\n",
      "   Non-cough samples: 0\n",
      "\n",
      "💾 Saved to training_data_final.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# First, let's create a mapping of filename to trial type\n",
    "def get_trial_type_from_filename(filename):\n",
    "    \"\"\"Extract trial type from filename\"\"\"\n",
    "    fname = filename.lower()\n",
    "    if 'no_talking' in fname:\n",
    "        return 'No_Talking'\n",
    "    elif 'talking' in fname:\n",
    "        return 'Talking'\n",
    "    elif 'nonverbal' in fname:\n",
    "        return 'Nonverbal'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Process in smaller batches\n",
    "batch_size = 100\n",
    "all_X = []\n",
    "all_y = []\n",
    "all_metadata = []\n",
    "\n",
    "print(\"Building training dataset in batches...\")\n",
    "print(f\"Total events to process: {len(events)}\")\n",
    "\n",
    "# Process events in batches\n",
    "for batch_start in tqdm(range(0, len(events), batch_size)):\n",
    "    batch_end = min(batch_start + batch_size, len(events))\n",
    "    batch_events = events[batch_start:batch_end]\n",
    "    \n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    metadata_batch = []\n",
    "    \n",
    "    for event in batch_events:\n",
    "        participant = event['participant']\n",
    "        filename = event['filename']\n",
    "        trial_type = get_trial_type_from_filename(filename)\n",
    "        \n",
    "        if trial_type is None:\n",
    "            continue\n",
    "        \n",
    "        # Load audio\n",
    "        audio_path = f\"dataset/{participant}/Trial_{trial_type}/{participant}_{trial_type}_In.wav\"\n",
    "        \n",
    "        if not os.path.exists(audio_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=750)\n",
    "            \n",
    "            # Get window around event (1 second before to 1 second after)\n",
    "            center = (event['start_time'] + event['end_time']) / 2\n",
    "            start_sample = int((center - 1.0) * sr)\n",
    "            end_sample = int((center + 1.0) * sr)\n",
    "            \n",
    "            # Handle boundaries\n",
    "            if start_sample < 0:\n",
    "                audio_segment = np.pad(audio[:end_sample], (-start_sample, 0))\n",
    "            elif end_sample > len(audio):\n",
    "                audio_segment = np.pad(audio[start_sample:], (0, end_sample - len(audio)))\n",
    "            else:\n",
    "                audio_segment = audio[start_sample:end_sample]\n",
    "            \n",
    "            # Extract mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_segment, \n",
    "                sr=sr, \n",
    "                n_mels=64,\n",
    "                n_fft=256, \n",
    "                hop_length=64\n",
    "            )\n",
    "            log_mel = librosa.power_to_db(mel_spec)\n",
    "            \n",
    "            X_batch.append(log_mel)\n",
    "            y_batch.append(1 if event['label'] == 'Cough' else 0)\n",
    "            metadata_batch.append({\n",
    "                'participant': participant,\n",
    "                'filename': filename,\n",
    "                'start_time': event['start_time'],\n",
    "                'end_time': event['end_time'],\n",
    "                'label': event['label']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save this batch\n",
    "    if X_batch:\n",
    "        all_X.extend(X_batch)\n",
    "        all_y.extend(y_batch)\n",
    "        all_metadata.extend(metadata_batch)\n",
    "        \n",
    "        # Save checkpoint every few batches\n",
    "        if len(all_X) % (batch_size * 5) == 0:\n",
    "            print(f\"\\nCheckpoint: {len(all_X)} samples processed so far\")\n",
    "            with open(f'training_data_checkpoint_{len(all_X)}.pkl', 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'X': np.array(all_X), \n",
    "                    'y': np.array(all_y), \n",
    "                    'metadata': all_metadata\n",
    "                }, f)\n",
    "\n",
    "# Convert to numpy arrays at the end\n",
    "X = np.array(all_X)\n",
    "y = np.array(all_y)\n",
    "\n",
    "print(f\"\\n✅ Dataset built successfully!\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Cough samples: {sum(y)}\")\n",
    "print(f\"   Non-cough samples: {len(y) - sum(y)}\")\n",
    "\n",
    "# Save final\n",
    "with open('training_data_final.pkl', 'wb') as f:\n",
    "    pickle.dump({'X': X, 'y': y, 'metadata': all_metadata}, f)\n",
    "\n",
    "print(\"\\n💾 Saved to training_data_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775c79a4-381e-41ef-9a5f-b7ad3da82559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 audio files with annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Processing: 005_no_talking_in.wav\n",
      "  ✅ Loaded audio: 787.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [00:01<00:03,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Processed 32 events from this file\n",
      "  💾 Saved checkpoint: checkpoint_005_no_talking_in.pkl\n",
      "\n",
      "📁 Processing: 006_No_Talking_Out.wav\n",
      "  ⚠️ File not found: dataset/006/Trial_1_No_Talking/006_No_Talking_In.wav\n",
      "\n",
      "📁 Processing: 006_Talking_In.wav\n",
      "  ✅ Loaded audio: 757.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:02<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Processed 210 events from this file\n",
      "  💾 Saved checkpoint: checkpoint_006_Talking_In.pkl\n",
      "\n",
      "✅ Final dataset: 242 samples\n",
      "   Cough samples: 70\n",
      "   Non-cough: 172\n",
      "\n",
      "💾 Saved final dataset to training_data_final.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#prepare for model training\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# First, load annotations\n",
    "with open(\"dataset/DataAnnotation.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Create a mapping of filename to all events in that file\n",
    "file_events = {}\n",
    "\n",
    "for ann in annotations:\n",
    "    filename = ann.get('original_filename', 'unknown')\n",
    "    participant = filename.split('_')[0]\n",
    "    \n",
    "    for seg in ann.get('segmentations', []):\n",
    "        for label_name in seg.get('annotations', {}).keys():\n",
    "            if filename not in file_events:\n",
    "                file_events[filename] = []\n",
    "            \n",
    "            file_events[filename].append({\n",
    "                'participant': participant,\n",
    "                'label': label_name,\n",
    "                'start_time': seg.get('start_time', 0),\n",
    "                'end_time': seg.get('end_time', 0)\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(file_events)} audio files with annotations\")\n",
    "\n",
    "# Process each file ONE AT A TIME\n",
    "all_X = []\n",
    "all_y = []\n",
    "all_metadata = []\n",
    "\n",
    "# Process only FIRST 3 files for testing (to avoid crash)\n",
    "files_to_process = list(file_events.keys())[:3]\n",
    "\n",
    "for filename in tqdm(files_to_process):\n",
    "    print(f\"\\n📁 Processing: {filename}\")\n",
    "    \n",
    "    # Get events for this file\n",
    "    file_events_list = file_events[filename]\n",
    "    participant = file_events_list[0]['participant']\n",
    "    \n",
    "    # Determine trial type from filename\n",
    "    fname_lower = filename.lower()\n",
    "    if 'no_talking' in fname_lower:\n",
    "        trial = 'No_Talking'\n",
    "        trial_folder = 'Trial_1_No_Talking'\n",
    "    elif 'talking' in fname_lower:\n",
    "        trial = 'Talking'\n",
    "        trial_folder = 'Trial_2_Talking'\n",
    "    elif 'nonverbal' in fname_lower:\n",
    "        trial = 'Nonverbal'\n",
    "        trial_folder = 'Trial_3_Nonverbal'\n",
    "    else:\n",
    "        print(f\"  ⚠️ Unknown trial type: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Load audio file (ONCE per file)\n",
    "    audio_path = f\"dataset/{participant}/{trial_folder}/{participant}_{trial}_In.wav\"\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"  ⚠️ File not found: {audio_path}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=750)\n",
    "        print(f\"  ✅ Loaded audio: {len(audio)/sr:.2f} seconds\")\n",
    "        \n",
    "        # Process each event in this file\n",
    "        for event in file_events_list:\n",
    "            # Get window around event\n",
    "            center = (event['start_time'] + event['end_time']) / 2\n",
    "            start_sample = int((center - 1.0) * sr)\n",
    "            end_sample = int((center + 1.0) * sr)\n",
    "            \n",
    "            # Handle boundaries\n",
    "            if start_sample < 0:\n",
    "                audio_segment = np.pad(audio[:end_sample], (-start_sample, 0))\n",
    "            elif end_sample > len(audio):\n",
    "                audio_segment = np.pad(audio[start_sample:], (0, end_sample - len(audio)))\n",
    "            else:\n",
    "                audio_segment = audio[start_sample:end_sample]\n",
    "            \n",
    "            # Extract mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio_segment, sr=sr, n_mels=64,\n",
    "                n_fft=256, hop_length=64\n",
    "            )\n",
    "            log_mel = librosa.power_to_db(mel_spec)\n",
    "            \n",
    "            all_X.append(log_mel)\n",
    "            all_y.append(1 if event['label'] == 'Cough' else 0)\n",
    "            all_metadata.append({\n",
    "                'file': filename,\n",
    "                'label': event['label'],\n",
    "                'start': event['start_time'],\n",
    "                'end': event['end_time']\n",
    "            })\n",
    "        \n",
    "        print(f\"  ✅ Processed {len(file_events_list)} events from this file\")\n",
    "        \n",
    "        # Save checkpoint after each file\n",
    "        checkpoint_file = f\"checkpoint_{filename.replace('.wav', '')}.pkl\"\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'X': np.array(all_X),\n",
    "                'y': np.array(all_y),\n",
    "                'metadata': all_metadata\n",
    "            }, f)\n",
    "        print(f\"  💾 Saved checkpoint: {checkpoint_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Final dataset: {len(all_X)} samples\")\n",
    "print(f\"   Cough samples: {sum(all_y)}\")\n",
    "print(f\"   Non-cough: {len(all_y) - sum(all_y)}\")\n",
    "\n",
    "# Save final\n",
    "with open('training_data_final.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X': np.array(all_X),\n",
    "        'y': np.array(all_y),\n",
    "        'metadata': all_metadata\n",
    "    }, f)\n",
    "\n",
    "print(\"\\n💾 Saved final dataset to training_data_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f07c8-0842-479c-9a92-0b08dfab34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the saved data\n",
    "with open('training_data_final.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print(f\"✅ Loaded dataset:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Cough samples: {sum(y)}\")\n",
    "print(f\"   Non-cough samples: {len(y) - sum(y)}\")\n",
    "\n",
    "# Reshape for CNN (add channel dimension)\n",
    "X_reshaped = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"📊 Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Build simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(64, X.shape[2], 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model and history\n",
    "model.save('cough_model.h5')\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "print(\"\\n💾 Model saved as 'cough_model.h5'\")\n",
    "print(\"💾 History saved as 'history.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4765d759-ecde-4481-a742-fa63298c0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded dataset:\n",
      "   X shape: (242, 64, 24)\n",
      "   y shape: (242,)\n",
      "   Cough samples: 70\n",
      "   Non-cough samples: 172\n",
      "   Flattened shape: (242, 1536)\n",
      "\n",
      "📊 Train set: 193 samples\n",
      "📊 Test set: 49 samples\n",
      "\n",
      "🌲 Training Random Forest...\n",
      "✅ Training complete!\n",
      "\n",
      "🎯 Results:\n",
      "   Accuracy: 0.959\n",
      "   AUC: 0.994\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Top 5 important features: [1379 1259 1283  467 1307]\n",
      "\n",
      "💾 Model saved as 'cough_model_rf.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the saved data\n",
    "with open('training_data_final.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print(f\"✅ Loaded dataset:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Cough samples: {sum(y)}\")\n",
    "print(f\"   Non-cough samples: {len(y) - sum(y)}\")\n",
    "\n",
    "# Flatten the spectrograms for sklearn\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "print(f\"   Flattened shape: {X_flat.shape}\")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_flat, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"📊 Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Train Random Forest (lightweight)\n",
    "print(\"\\n🌲 Training Random Forest...\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"✅ Training complete!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\"\\n🎯 Results:\")\n",
    "print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   AUC: {auc:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = model.feature_importances_\n",
    "print(f\"\\n📊 Top 5 important features: {np.argsort(importances)[-5:]}\")\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, 'cough_model_rf.pkl')\n",
    "print(\"\\n💾 Model saved as 'cough_model_rf.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd3a11-73fc-4145-8cfe-6bacd56661a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba4260-52a8-4ec5-b429-d3e60620e803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
